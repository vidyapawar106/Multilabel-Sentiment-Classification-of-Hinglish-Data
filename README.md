# Multilabel-Sentiment-Classification-of-Hinglish-Data
With an increase in the number of developments in NLP domain today, pre-trained language representations have become the most popular technique for various downstream language tasks among the NLP practitioners. These pre-trained models or word embeddings have been trained using unsupervised training on a large amount of different English text corpus for different NLP tasks like Neural Machine Translation, Parts of Speech Tagging, Relation Extraction, Name Entity Recognition, Information Retrieval, Question & Answering, Text Classification, Text Summarization and so on. Our objective is to compare the pre-trained models like BERT, RoBERTa, DistilBERT, AlBERT and XLNet by performing multilabel sentiment classification of Hinglish (Hindi in English Script) data of Youtube comments collected from the cooking/catering domain. These comments are classified into 7 labels like Gratitude, About the recipe, About the video, Praising, Hybrid, Undefined and Suggestions & queries. We have used the Simple Transformers library which is built on top of the Hugging Faceâ€™s Transformers library for supervised training of the model. It is an easy and ready to use the library as it includes most of the current pre-trained models which can be used to train data from any domain with just 3 lines of code without training the model from scratch. We have done fine-tuning of 4 hyperparameters namely, maximum sequence length, train and evaluation batch size, learning rates and number of training epochs for each of the pre-trained models. Our experiment and results show that traditional BERT is superior to the other pre-trained models in terms of achieving state-of-the-art performance.
